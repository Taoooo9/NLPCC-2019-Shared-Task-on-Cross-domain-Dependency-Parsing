[Data]
pretrained_embeddings_file = emb/embedding.300
# pretrained_embeddings_file = ../emb/giga.100.txt.sample
# data_dir = ../ctb51
data_dir = NLPCC2019
train_file = BC/BC-Train.conll
train_target_file = PB/PB-Train.conll
dev_file = PB/PB-Dev.conll
test_file = PB/PB-Dev.conll
unlabelled_file = PB-Unlabeled.conll 
min_occur_count = 2

[Save]
save_dir = pb-parser-model-10
config_file = %(save_dir)s/config.cfg
save_model_path = %(save_dir)s/model
save_char_model_path = %(save_dir)s/char_model
save_char_vocab_path = %(save_dir)s/char_vocab
save_classifier_model_path = %(save_dir)s/classifier_model
save_vocab_path = %(save_dir)s/vocab
load_dir = pb-parser-model-6
load_model_path = %(load_dir)s/model
load_char_model_path = %(load_dir)s/char_model
load_classifier_model_path = %(load_dir)s/classifier_model
load_vocab_path = %(load_dir)s/vocab
load_char_vocab_path = %(load_dir)s/char_vocab

[Network]
use_char = True
lstm_layers = 3
word_dims = 300
char_dims = 100
tag_dims = 100
dropout_emb = 0.33
lstm_hiddens = 400
dropout_lstm_input = 0.33
dropout_lstm_hidden = 0.33
mlp_arc_size = 500
mlp_rel_size = 100
dropout_mlp = 0.33
start_adversarial = 1000000

[Optimizer]
alpha = 1e-5
learning_rate = 2e-3
classifier_learning_rate = 1e-3
decay = .75
decay_steps = 500
beta_1 = .9
beta_2 = .9
epsilon = 1e-12
clip = 5.0

[Run]
num_buckets_train = 40
num_buckets_valid = 10
num_buckets_test = 10
train_iters = 50000
train_batch_size = 50
test_batch_size = 50
validate_every = 200
save_after = 0
update_every = 4
threshold = 0.7

